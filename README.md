#  Policy Analysis with NLP

### 1. Introduction

In order to achieve wide-scale implementation of climate targets, the values and priorities of multi-sectoral stakeholders must be aligned and integrated with other development agenda. Although misalignment across scales of policy and between stakeholders are well-known barriers to implementing restoration, fast-paced policy making in multi-stakeholder environments complicates the monitoring and the analysis of governance and policy. As the World Resources Institute (WRI) aims to facilitate sustainable development of environment, our team plans to utilize NLP and machine learning to monitor worldwide restoration initiatives and detect incentives and disincentives across diverse policy documents.

### 2. Frameworks

The current work of WRI focuses on converting the policy documents in plain text into datasets with labels indicating the financial incentives and disincentives in each sentence, which helps them gain a deeper understanding about the worldwide land restoration. This workflow includes two key methods: Label Model and Neural Networks. In the Label Model method, the manually labeled gold standard dataset was used to train the Snorkel Label Model, resulting in a trained model and a new dataset with labels generated by Snorkel. Then the trained model was used to predict labels for unlabeled samples to generate a noisy dataset. After that, the workflow enters the Neural Networks method, where the sentences in both datasets are encoded by roBERTa and the gold standard dataset was first split into training and validation data to train a shallow recurrent neural network. Then the noisy dataset was used as training data to train another shallow RNN as well. Therefore, the current workflow eventually generated two trained neural network models and one of them serves as the baseline model.

In order to optimize the current workflow, our team will be focusing on Label Model optimization and Neural Networks optimization. In the Label Model optimization, our team is planning to compare the performance of Snorkel Model and BabbleLabble. In the Neural Networks optimization, our team is expecting to build a deeper and more complicated neural network and fine tune roBERTa encoding as well. Meanwhile, our team will also utilize additional feature engineering for both models to further improve their performance.


### 3. Benefits

In order to attain worldwide execution of land restoration, the priorities and responsibilities of international environmental agencies must be coordinated and aligned. Unfortunately, there are some remaining huge gaps between the policies and each country’s audience, and the rapid legislative development worldwide further complicates the situation. Thus, how to popularize the policy content to the audience, and support the implementation of the policy becomes one of thebiggest business problems that the restoration project currently face. WRI seeks to utilize technical tools including NLP and machine learning to resolve the challenge and how to build a model framework for accurately analyzing the policy documents is one of the most essential problems for the organization right now. Therefore, the two optimization tasks that our team is focusing on will bring benefits to WRI in the following three ways.

First, by optimizing the key methods in the current workflow, our team is expecting to generate a more accurate model which can classify the policy documents from different countries based on the financial incentives and disincentives it provides for land restoration. This improved model will help WRI understand whether each policy document supports land restoration in a more efficient way, leading to more effective and positive impacts on the farmers community. Second, the classification model can be applied in a more global scale in the future as it can be used to analyze the policy documents of different countries. Furthermore, by modifying the current language modeling and encoding method, WRI can apply this model to classify documents in other languages as well.

Third, more aspects of each financing mechanism can also be identified utilizing the classification model (neural networks) through transfer learning. For example, the model can be adjusted to identify additional characteristics of the financing policy including type, audience, financier, stipulations, and amount. In this way, the organization will be able to understand and extract the information of the financial mechanisms in policy documents automatically, which can not only help popularize the policy content to the general audience more efficiently but also enhance the implementation of each policy as the audience gains a better and deeper understanding of it.


### 4. Methodologies Illustration

**Labeling Model Optimization:**

In the current Snorkel model that WRI trained, they listed some important words that express different emotions, and considered them as well as the relative position of these words as main features to write the functions and train the model. To enhance the performance of the Snorkel, our team thinks that more features, including N-grams, Part-of-Speech tags generated by Hidden Markov Model, LIWC Features, Named Entity Features, Word Frequency and similarity coefficient, should also be added as features to the Snorkel model.

In order to prove that adding more features can enhance the performance of Snorkel model, our team plans to select some samples with different labels from our training data firstly and do a quick calculation. If there are significant differences among all the three categories in terms of the features that our team selects, it can be concluded that the features have high possibility to affect the labeling results, and should be added to the model.

For example, our team selected three “positive” sentences, three “negative” sentences, and three “neutral” sentences from the training data, and calculated the similarity matrix. The hypothesis our team set was that the similarity coefficient in the same class should be larger than the similarity coefficient between different classes. Our team applied TFIDF (term frequency–inverse document frequency) with cosine similarity, as well as Google Word2Vec model with cosine similarity to do the calculation respectively, and their results are shown as follows. Based on these results, it can be concluded that the average similarity coefficient between two classes is smaller than the average similarity coefficient in each one class, which means that the similarity coefficient differs in different classes and it can be used as a feature tohelp with the labeling model. Then, some “positive” sentences are selected as well as some “negative” sentences as baselines, and calculate their similarity coefficients with other sentences that we want to label.

For the other features that were mentioned before, our team should also test them in a similar way, and use all the features that has been proved to be useful to train our final Snorkel model. After that, our team will compare the new results with initial results that WRI provided to see if our feature engineering successfully help with the Labeling Model Optimization.

In addition to Snorkel, BabbleLabble is another model that our team can use for labeling. The working function of BabbleLabble approach is to use semantic parser to convert natural language explanations into labeling functions that are “accurate enough” for using with data programming. In other words, BabbleLabble converts natural language (babble) into labels. In order to use BabbleLabble approach effectively, our team needs to first set up explanations. For example, we would write an explanation like “any policy that contain support is an incentive policy” and put this explanation into BabbleLabble. Then BabbleLabble would convert this explanation into codes, which could be further applied in machine learning procedures.

Currently, our team has successfully ran BabbleLabble following the tutorial on GitHub, so the next step is to try it to generate noisy label. Our team is at the stage of adding explanations to BabbleLabble, where meaningful and unbiased explanations need to be selected in order to generate a better and more accurate labeling function. Then, our team would like to compare the overall performance of BabbleLabble and Snorkel, select the one with higher accuracy and then further improve its performance for an even better result.

**Neural Networks Optimization:**

Building a neural network model is the key approach for our team to predict and classify the policy documents based on their financial incentives and disincentives regarding land restoration.

● Develop Deeper and More Complicated Neural Networks The current neural network that WRI uses is a shallow RNN which includes only a LSTM layer and a fully-connected layer. In order to improve the performance, our team have tried adding an embedding layer and change the LSTM layer into a plain RNN layer or a GRU laye and the loss function results on the validation dataset showed a small improvement. Although we will need to validate this improvement in prediction accuracy by introducing a test dataset and calculating the F1 score, the current results show that it is feasible to improve the model performance by adding more layers with more kinds of functions. The final neural network that our team is designing may include additional embedding layer, dense layers, plain RNN layers, LSTM layers and GRU layers.
● Fine tune RoBERTa Encoding Method Our team has run the roBERTa encoding method locally on some policy samples, which generated a three-dimensional matrix which can be used as one of the features in the RNN model. The current pretrained roBERTa model that WRI is using is roberta.large and our team is planning to try other alternatives like roberta.large.mnli which is the roBERTa large model fine-tuned on the Multi-Genre NLI Corpus, roberta.large.wsc which is fine-tuned on Winograd Schema Challenge data, and other roBERTa models our team fine-tuned on different corpus or datasets.

● Additional Feature Engineering The current shallow RNN takes the roBERTa encoding of sentences as the input and outputs the predicted probabilities of three labels: positive, negative, and neutral. In order to optimize the performance of the RNN, adding additional features is an available resource which include N-grams, Part-of-Speech tags generated by Hidden Markov Model, LIWC Features, Named Entity Features, Word Frequency and similarity coefficient.

This neural networks method will generate two models including one baseline model trained by the gold standard dataset and another model trained by the noisy dataset. Whether the F1 score of the second model is higher than that of the baseline model with a statistically significant difference will indicate whether our trained model is accurate enough to perform the classification. Therefore, the difference between the two models’ F1 scores should be regarded as one of the criteria for success in this project.
